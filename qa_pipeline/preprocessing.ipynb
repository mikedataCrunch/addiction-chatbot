{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57473b2b",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03b540d",
   "metadata": {},
   "source": [
    "## Filtering URLs\n",
    "\n",
    "Crawling step must account for search results that do not span the page range inputted for crawling. Example: General conference results for the *addiction recovery* search phrase only returns one (1) page but the crawler still stretches out the crawl to some total number of pages to crawl.\n",
    "\n",
    "In this step, we fix the returned urls by way of filtering out the duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8d5069d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-30T18:24:43.502906Z",
     "start_time": "2022-03-30T18:24:43.496661Z"
    }
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import tokenize\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4c93a2ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-30T18:24:43.783569Z",
     "start_time": "2022-03-30T18:24:43.774647Z"
    }
   },
   "outputs": [],
   "source": [
    "def filter_dup_urls(source_file, dest_dir, save_file, inspect=True): \n",
    "    with open(source_file, 'r') as f:\n",
    "        url_list = json.load(f)\n",
    "    url_list = [i[2] for i in url_list]\n",
    "    filtered_url_list = sorted(list(set(url_list)))\n",
    "\n",
    "    if not os.path.exists(dest_dir):\n",
    "        os.mkdir(dest_dir)\n",
    "    save_path = os.path.join(dest_dir, save_file)\n",
    "    with open(save_path, 'w') as f:\n",
    "        json.dump(filtered_url_list, f)\n",
    "    if inspect:\n",
    "        return filtered_url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1e305953",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-30T18:24:44.226915Z",
     "start_time": "2022-03-30T18:24:44.201557Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New length:  13\n"
     ]
    }
   ],
   "source": [
    "source_file = '../general-conference-1-1000.json'\n",
    "dest_dir = '../data/urls'\n",
    "save_file = 'general-conference-urls.json'\n",
    "\n",
    "gc_urls = filter_dup_urls(source_file, dest_dir, save_file)\n",
    "print(\"New length: \", len(gc_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c9983e31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-30T18:24:45.053626Z",
     "start_time": "2022-03-30T18:24:45.018214Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New length:  209\n"
     ]
    }
   ],
   "source": [
    "source_file = '../magazines-1-1000.json'\n",
    "dest_dir = '../data/urls'\n",
    "save_file = 'magazines-urls.json'\n",
    "\n",
    "mag_urls = filter_dup_urls(source_file, dest_dir, save_file)\n",
    "print(\"New length: \", len(mag_urls))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f05139",
   "metadata": {},
   "source": [
    "## Filter scraped data\n",
    "\n",
    "In this step, we filter the already scraped data to avoid the scraping step again.\n",
    "\n",
    "* Remove duplicates\n",
    "* Remove non-english text based on `?lang=eng`\n",
    "* Remove files with no body text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1a7f6122",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-30T18:33:15.275920Z",
     "start_time": "2022-03-30T18:33:15.262183Z"
    }
   },
   "outputs": [],
   "source": [
    "def filter_scraped_data(source_dir, dest_dir, inspect=True):\n",
    "    \"\"\"Discards json files with no body text, exclude duplicates, also filters non-english urls\"\"\"\n",
    "    files = glob(os.path.join(source_dir, '*.json'))\n",
    "    \n",
    "    urls_filtered = []\n",
    "    no_body = []\n",
    "    non_english = []\n",
    "    for file in tqdm(files):\n",
    "        with open(file) as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        if data['url'] in urls_filtered:\n",
    "            continue\n",
    "            \n",
    "        # check if english\n",
    "        if \"?lang=\" in data['url']:\n",
    "            if data['url'].split(\"?lang=\")[1][:3] != \"eng\":\n",
    "                non_english.append(data['url'])\n",
    "                continue\n",
    "            \n",
    "        if data['body']:\n",
    "            urls_filtered.append(data['url'])\n",
    "            filename = file.split('/')[-1]\n",
    "            if not os.path.exists(dest_dir):\n",
    "                os.makedirs(dest_dir)\n",
    "            save_path = os.path.join(dest_dir, filename)\n",
    "            with open(save_path, 'w') as f:\n",
    "                json.dump(data, f)\n",
    "        else:\n",
    "            no_body.append(data)\n",
    "            \n",
    "    if inspect:\n",
    "        return urls_filtered, no_body, non_english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "205c8ba3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-30T18:33:26.333371Z",
     "start_time": "2022-03-30T18:33:21.549056Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8965/8965 [00:04<00:00, 1890.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New length:  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "source_dir = '../general-conference'\n",
    "dest_dir = '../data/filtered/general-conference'\n",
    "gc_urls_new, gc_nb, gc_noneng = filter_scraped_data(source_dir, dest_dir)\n",
    "print(\"New length: \", len(gc_urls_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3986bcb9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-30T18:33:33.105781Z",
     "start_time": "2022-03-30T18:33:33.098404Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'https://www.churchofjesuschrist.org/study/general-conference/1981/10/_manifest?lang=eng',\n",
       " 'https://www.churchofjesuschrist.org/study/general-conference/1981/10?lang=eng',\n",
       " 'https://www.churchofjesuschrist.org/study/general-conference/2007/10/blessed-are-all-the-pure-in-heart?lang=ase'}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of filtered out urls\n",
    "(set(gc_urls) - set(gc_urls_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9b00b7e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-30T18:35:02.773214Z",
     "start_time": "2022-03-30T18:34:56.648723Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9978/9978 [00:06<00:00, 1640.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New length:  177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "source_dir = '../magazines'\n",
    "dest_dir = '../data/filtered/magazines'\n",
    "mag_urls_new, mag_nb, mag_noneng = filter_scraped_data(source_dir, dest_dir)\n",
    "print(\"New length: \", len(mag_urls_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "79e533d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-30T18:35:14.302419Z",
     "start_time": "2022-03-30T18:35:14.295378Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of filtered out urls\n",
    "len(set(mag_urls) - set(mag_urls_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e79a3c4",
   "metadata": {},
   "source": [
    "## Transform to SQuAD flavor\n",
    "\n",
    "In this task, we create the Q&A dataset by parsing each body text to find a question sentence, an answer sentence, and a context.\n",
    "\n",
    "Assumptions:\n",
    "\n",
    "1. The sentence following the question sentence is the answer.\n",
    "2. To mimic popular Q&A datasets, we also need to input a starting index, and a context. See figure that follows.\n",
    "3. The context is defined as the sentences before a questions if it exists, and the sentences after the question. Including the answer sentence. The choices for how many sentances to take in from before or after a question is arbitrary.\n",
    "\n",
    "The format we are going for is shown below:\n",
    "\n",
    "<img src=\"images/squad_format.png\"></img>\n",
    "<center>Figure 1. A snippert of the SQuAD dataset</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "07fc686f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-30T18:41:40.254234Z",
     "start_time": "2022-03-30T18:41:40.236533Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_qa(source_dir, dest_dir, context_size=5, answer_size=1):\n",
    "    \"\"\"Expand json file into body paragraphs\"\"\"\n",
    "    files = glob(os.path.join(source_dir, '*.json'))\n",
    "    for file in tqdm(files):\n",
    "        with open(file) as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        paragraphs = data['body'].split('\\n')\n",
    "        sentences = []\n",
    "        for paragraph in paragraphs:\n",
    "            sentences.extend(tokenize.sent_tokenize(paragraph))\n",
    "        question_indices = []\n",
    "        for index, sentence in enumerate(sentences):\n",
    "            if sentence.endswith('?'):\n",
    "                question_indices.append(index)\n",
    "        s_length = len(sentences)\n",
    "        for q in question_indices:\n",
    "            qa_data = {k:v for k,v in data.items() if k not \\\n",
    "                       in ['kicker', 'body', 'author', 'calling', 'url']}    \n",
    "            \n",
    "            # right context\n",
    "            if q + answer_size >= len(sentences): # last sentence of the body\n",
    "                continue\n",
    "            elif q + 5 < len(sentences):\n",
    "                context_right = ' '.join(sentences[q + 1: q + 6])\n",
    "            else:\n",
    "                # sentence after, then onwards\n",
    "                context_right = ' '.join(sentences[q + 1: ])\n",
    "            \n",
    "            # left context\n",
    "            if q == 0:\n",
    "                context_left = None\n",
    "            elif q - 5 >= 0:\n",
    "                context_left = ' '.join(sentences[q - 5: q])\n",
    "            else:\n",
    "                context_left = ' '.join(sentences[0: q])\n",
    "            \n",
    "            \n",
    "            # build context\n",
    "            if bool(context_left):\n",
    "                context = context_left + ' ' + context_right\n",
    "                answer_start = len(context_left) + 1\n",
    "            else:\n",
    "                context = context_right\n",
    "                answer_start = len(sentences[q])\n",
    "            \n",
    "            # build qa_data\n",
    "            qa_data['context'] = context\n",
    "            qa_data['question'] = sentences[q]\n",
    "            \n",
    "            # get answer indices : the next answer_size sentences to the question\n",
    "            answer_index = [q + 1]\n",
    "            while answer_index[-1] != q + answer_size:\n",
    "                answer_index.append(answer_index[-1] + 1)\n",
    "            answer_list = list(np.array(sentences)[answer_index])\n",
    "            if len(answer_list) > 1:\n",
    "                text = ' '.join(answer_list)\n",
    "            else:\n",
    "                text = answer_list[0]\n",
    "            answers = {\n",
    "                'answer_start': [answer_start],\n",
    "                'text': [text]\n",
    "            }\n",
    "            qa_data['answers'] = answers\n",
    "            if not os.path.exists(dest_dir):\n",
    "                os.makedirs(dest_dir)\n",
    "            new_filename = str(uuid.uuid4()) + '.json'    \n",
    "            save_path = os.path.join(dest_dir, new_filename)\n",
    "            with open(save_path, 'w') as f:\n",
    "                json.dump(qa_data, f)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ae4ec278",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-30T18:52:24.029526Z",
     "start_time": "2022-03-30T18:52:20.832587Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 53.58it/s]\n",
      "100%|██████████| 177/177 [00:00<00:00, 182.53it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 66.62it/s]\n",
      "100%|██████████| 177/177 [00:00<00:00, 194.35it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 68.76it/s]\n",
      "100%|██████████| 177/177 [00:00<00:00, 221.45it/s]\n"
     ]
    }
   ],
   "source": [
    "facets = ['general-conference', 'magazines']\n",
    "answer_sizes = [1, 2, 3] # sentences after the question sentence\n",
    "\n",
    "for answer_size in answer_sizes:\n",
    "    for facet in facets:\n",
    "        source_dir = f'../data/filtered/{facet}'\n",
    "        dest_dir = f'../data/qa-{answer_size}/{facet}'\n",
    "        extract_qa(source_dir, dest_dir, answer_size=answer_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f9938c",
   "metadata": {},
   "source": [
    "## Expanding to paragraph level documents for LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "96e86b4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-30T18:52:36.055925Z",
     "start_time": "2022-03-30T18:52:36.040765Z"
    }
   },
   "outputs": [],
   "source": [
    "def expand(source_dir, dest_dir, facet):\n",
    "    \"\"\"Expand json file into body paragraphs\"\"\"\n",
    "    files = glob(os.path.join(source_dir, '*.json'))\n",
    "    for file in tqdm(files):\n",
    "        with open(file) as f:\n",
    "            data = json.load(f)\n",
    "        for index, paragraph in enumerate(data['body'].split('\\n')):\n",
    "            paragraph_data = {k:v for k,v in data.items() if k != 'body'}\n",
    "            paragraph_data['body'] = paragraph\n",
    "            paragraph_data['paragraph_index'] = index\n",
    "            new_filename = f\"{facet}-index-{paragraph_data['index']}-paragraph-{index}.json\"\n",
    "            save_dir = os.path.join(dest_dir, facet)\n",
    "            if not os.path.exists(save_dir):\n",
    "                os.makedirs(save_dir)\n",
    "            save_path = os.path.join(save_dir, new_filename)\n",
    "            with open(save_path, 'w') as f:\n",
    "                json.dump(paragraph_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ee09ac37",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-30T18:52:42.803459Z",
     "start_time": "2022-03-30T18:52:37.512957Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 25.79it/s]\n",
      "100%|██████████| 177/177 [00:04<00:00, 36.23it/s]\n"
     ]
    }
   ],
   "source": [
    "for facet in facets:\n",
    "    source_dir = f'../data/filtered/{facet}'\n",
    "    dest_dir = '../data/expanded'\n",
    "    expand(source_dir, dest_dir, facet)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
