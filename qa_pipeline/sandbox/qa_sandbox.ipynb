{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27ae8202",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-25T18:15:27.983898Z",
     "start_time": "2022-03-25T18:15:25.543573Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "from glob import glob\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "353f3725",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-25T18:15:28.697382Z",
     "start_time": "2022-03-25T18:15:27.986454Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mdorosan/2022/addiction_chatbot/qa_pipeline\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15035e7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-25T18:15:28.856474Z",
     "start_time": "2022-03-25T18:15:28.701186Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.15.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04d2fbc2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-25T18:15:28.938982Z",
     "start_time": "2022-03-25T18:15:28.858246Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login successful\n",
      "Your token has been saved to /home/mdorosan/.huggingface/token\n",
      "\u001b[1m\u001b[31mAuthenticated through git-credential store but this isn't the helper defined on your machine.\n",
      "You might have to re-authenticate when pushing to the Hugging Face Hub. Run the following command in your terminal in case you want to set this credential helper as the default\n",
      "\n",
      "git config --global credential.helper store\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fe45476a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-25T18:24:14.852505Z",
     "start_time": "2022-03-25T18:24:10.393089Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting git-lfs\n",
      "  Downloading git_lfs-1.6-py2.py3-none-any.whl (5.6 kB)\n",
      "Installing collected packages: git-lfs\n",
      "Successfully installed git-lfs-1.6\n"
     ]
    }
   ],
   "source": [
    "# !pip install git-lfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0ffd28",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c5af476",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-25T18:16:29.236472Z",
     "start_time": "2022-03-25T18:16:07.579602Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d199186f362479e9cc2bcf610944fb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/26917 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-ae0363a304eda62b\n",
      "Reusing dataset json (/home/mdorosan/.cache/huggingface/datasets/json/default-ae0363a304eda62b/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03fc72fe01344696a8b309587ab45dff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.3 s, sys: 9.96 s, total: 28.2 s\n",
      "Wall time: 21.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_files = {'../data/qa-2/general-conference/*.json'}\n",
    "ar = load_dataset(\"json\", data_files=data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81aeacdc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-25T18:16:29.875947Z",
     "start_time": "2022-03-25T18:16:29.238927Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['page', 'index', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 21533\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['page', 'index', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 5384\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split\n",
    "ar = ar['train'].train_test_split(test_size=0.2)\n",
    "\n",
    "# inspect dataset\n",
    "ar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e942277c",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1077fcf5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-25T18:16:40.301149Z",
     "start_time": "2022-03-25T18:16:29.877778Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "251987e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-25T18:16:40.307907Z",
     "start_time": "2022-03-25T18:16:40.304423Z"
    }
   },
   "outputs": [],
   "source": [
    "# checlk if tokenizer is fast tokenizer\n",
    "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9994c88c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-25T18:16:40.320336Z",
     "start_time": "2022-03-25T18:16:40.310022Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    \"\"\"Understand this!\"\"\"\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=384,\n",
    "        truncation=\"only_second\",\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        answer = answers[i]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label it (0, 0)\n",
    "        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d63d4ef5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-25T18:16:52.879040Z",
     "start_time": "2022-03-25T18:16:40.322468Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39b0f447e6494ba585af2e2013833ec8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeb6c0c4d83a4f86a7269d1d37ef894a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 16s, sys: 45.6 ms, total: 1min 16s\n",
      "Wall time: 12.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenized_ar = ar.map(preprocess_function, batched=True, remove_columns=ar[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5dc09a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-25T18:16:55.787500Z",
     "start_time": "2022-03-25T18:16:52.881299Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] do you realize how deeply the lord loves and cherishes you, even now? [SEP] they need to know the dangers of pornography and how it overtakes lives, causing loss of the spirit, distorted feelings, deceit, damaged relationships, loss of self - control, and nearly total consumption of time, thought, and energy. pornography is more vile, evil, and graphic than ever before. as we counsel with our children, together we can create a family plan with standards and boundaries, being proactive to protect our homes with filters on electronic devices. parents, are we aware that mobile devices with internet capacity, not computers, are the biggest culprit? 2 young people and adults, if you are caught in satan ’ s trap of pornography, remember how merciful our beloved savior is. our savior has the power to cleanse and heal you. he can remove the pain and sorrow you feel and make you clean again through the power of his atonement. we as leaders are also greatly concerned about the spouses and families of those suffering from pornography addiction. elder richard g. scott has pleaded : “ if you are free of serious sin yourself, don ’ t suffer needlessly the consequences of another ’ s sins.... you can feel compassion. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect\n",
    "x = tokenized_ar[\"train\"][0][\"input_ids\"]\n",
    "tokenizer.decode(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "209d51a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-25T18:16:55.968512Z",
     "start_time": "2022-03-25T18:16:55.789613Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import DefaultDataCollator\n",
    "\n",
    "data_collator = DefaultDataCollator(return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70ffa3ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-25T18:16:58.413621Z",
     "start_time": "2022-03-25T18:16:55.970523Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-26 02:16:56.263162: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-26 02:16:57.888205: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10419 MB memory:  -> device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:04:00.0, compute capability: 6.1\n",
      "2022-03-26 02:16:57.889954: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 10419 MB memory:  -> device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:84:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "tf_train_set = tokenized_ar[\"train\"].to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"start_positions\", \"end_positions\"],\n",
    "    dummy_labels=True,\n",
    "    shuffle=True,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "tf_validation_set = tokenized_ar[\"test\"].to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"start_positions\", \"end_positions\"],\n",
    "    dummy_labels=True,\n",
    "    shuffle=False,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "292c2ff2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-25T18:16:58.874767Z",
     "start_time": "2022-03-25T18:16:58.416610Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import create_optimizer\n",
    "\n",
    "batch_size = 16\n",
    "num_epochs = 2\n",
    "total_train_steps = (len(tokenized_ar[\"train\"]) // batch_size) * num_epochs\n",
    "\n",
    "optimizer, schedule = create_optimizer(\n",
    "    init_lr=2e-5,\n",
    "    num_warmup_steps=0,\n",
    "    num_train_steps=total_train_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c91d7b8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-25T18:17:02.712878Z",
     "start_time": "2022-03-25T18:16:58.876648Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-26 02:17:01.737807: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForQuestionAnswering: ['vocab_layer_norm', 'vocab_transform', 'vocab_projector', 'activation_13']\n",
      "- This IS expected if you are initializing TFDistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs', 'dropout_19']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForQuestionAnswering\n",
    "model = TFAutoModelForQuestionAnswering.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "087e4d76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-25T18:17:02.727225Z",
     "start_time": "2022-03-25T18:17:02.715055Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! Please ensure your labels are passed as the 'labels' key of the input dict so that they are accessible to the model during the forward pass. To disable this behaviour, please pass a loss argument, or explicitly pass loss=None if you do not want your model to compute a loss.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3eb3d818",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-25T18:17:02.737664Z",
     "start_time": "2022-03-25T18:17:02.729147Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_distil_bert_for_question_answering\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "distilbert (TFDistilBertMain multiple                  66362880  \n",
      "_________________________________________________________________\n",
      "qa_outputs (Dense)           multiple                  1538      \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         multiple                  0         \n",
      "=================================================================\n",
      "Total params: 66,364,418\n",
      "Trainable params: 66,364,418\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b6b3f941",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-25T18:41:30.486542Z",
     "start_time": "2022-03-25T18:32:11.703774Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-26 02:32:11.712298: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2022-03-26 02:32:11.712396: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
      "2022-03-26 02:32:11.712689: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1666] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.\n",
      "2022-03-26 02:32:11.712810: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
      "2022-03-26 02:32:11.712872: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1757] function cupti_interface_->Finalize()failed with error CUPTI could not be loaded or symbol could not be found.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "   1/1345 [..............................] - ETA: 9:46 - loss: 5.3270"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-26 02:32:12.449867: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2022-03-26 02:32:12.449939: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
      "2022-03-26 02:32:12.450059: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1666] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "   2/1345 [..............................] - ETA: 12:43 - loss: 5.3740"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-26 02:32:12.829865: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
      "2022-03-26 02:32:12.830018: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1757] function cupti_interface_->Finalize()failed with error CUPTI could not be loaded or symbol could not be found.\n",
      "2022-03-26 02:32:12.843349: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 0 callback api events and 0 activity events. \n",
      "2022-03-26 02:32:12.849914: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
      "2022-03-26 02:32:12.854480: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: ./qa_model_save/logs/train/plugins/profile/2022_03_26_02_32_12\n",
      "\n",
      "2022-03-26 02:32:12.856553: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to ./qa_model_save/logs/train/plugins/profile/2022_03_26_02_32_12/jupyter-mdorosan.trace.json.gz\n",
      "2022-03-26 02:32:12.891060: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: ./qa_model_save/logs/train/plugins/profile/2022_03_26_02_32_12\n",
      "\n",
      "2022-03-26 02:32:12.900510: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to ./qa_model_save/logs/train/plugins/profile/2022_03_26_02_32_12/jupyter-mdorosan.memory_profile.json.gz\n",
      "2022-03-26 02:32:12.905836: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: ./qa_model_save/logs/train/plugins/profile/2022_03_26_02_32_12\n",
      "Dumped tool data for xplane.pb to ./qa_model_save/logs/train/plugins/profile/2022_03_26_02_32_12/jupyter-mdorosan.xplane.pb\n",
      "Dumped tool data for overview_page.pb to ./qa_model_save/logs/train/plugins/profile/2022_03_26_02_32_12/jupyter-mdorosan.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to ./qa_model_save/logs/train/plugins/profile/2022_03_26_02_32_12/jupyter-mdorosan.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to ./qa_model_save/logs/train/plugins/profile/2022_03_26_02_32_12/jupyter-mdorosan.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to ./qa_model_save/logs/train/plugins/profile/2022_03_26_02_32_12/jupyter-mdorosan.kernel_stats.pb\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1345/1345 [==============================] - 559s 415ms/step - loss: 0.0961 - val_loss: 3.6605e-04\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Saving the model to HDF5 format requires the model to be a Functional model or a Sequential model. It does not work for subclassed models, because such models are defined via the body of a Python method, which isn't safely serializable. Consider saving to the Tensorflow SavedModel format (by setting save_format=\"tf\") or using `save_weights`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_582214/2870521182.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# portless : 7 mins for each epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m model.fit(\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf_train_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf_validation_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda-envs/msds2022-ml3/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1228\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1230\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1231\u001b[0m         \u001b[0mtraining_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda-envs/msds2022-ml3/lib/python3.9/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    411\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 413\u001b[0;31m       \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda-envs/msds2022-ml3/lib/python3.9/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m   1366\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_freq\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'epoch'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1368\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_should_save_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda-envs/msds2022-ml3/lib/python3.9/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_save_model\u001b[0;34m(self, epoch, batch, logs)\u001b[0m\n\u001b[1;32m   1420\u001b[0m                     filepath, overwrite=True, options=self._options)\n\u001b[1;32m   1421\u001b[0m               \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1422\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1423\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda-envs/msds2022-ml3/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filepath, overwrite, include_optimizer, save_format, signatures, options, save_traces)\u001b[0m\n\u001b[1;32m   2143\u001b[0m     \"\"\"\n\u001b[1;32m   2144\u001b[0m     \u001b[0;31m# pylint: enable=line-too-long\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2145\u001b[0;31m     save.save_model(self, filepath, overwrite, include_optimizer, save_format,\n\u001b[0m\u001b[1;32m   2146\u001b[0m                     signatures, options, save_traces)\n\u001b[1;32m   2147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda-envs/msds2022-ml3/lib/python3.9/site-packages/keras/saving/save.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer, save_format, signatures, options, save_traces)\u001b[0m\n\u001b[1;32m    136\u001b[0m     if (not model._is_graph_network and  # pylint:disable=protected-access\n\u001b[1;32m    137\u001b[0m         not isinstance(model, sequential.Sequential)):\n\u001b[0;32m--> 138\u001b[0;31m       raise NotImplementedError(\n\u001b[0m\u001b[1;32m    139\u001b[0m           \u001b[0;34m'Saving the model to HDF5 format requires the model to be a '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m           \u001b[0;34m'Functional model or a Sequential model. It does not work for '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Saving the model to HDF5 format requires the model to be a Functional model or a Sequential model. It does not work for subclassed models, because such models are defined via the body of a Python method, which isn't safely serializable. Consider saving to the Tensorflow SavedModel format (by setting save_format=\"tf\") or using `save_weights`."
     ]
    }
   ],
   "source": [
    "from transformers.keras_callbacks import PushToHubCallback\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "\n",
    "\n",
    "tensorboard_callback = TensorBoard(log_dir=\"./qa_model_save/logs\")\n",
    "\n",
    "callbacks = [tensorboard_callback]\n",
    "\n",
    "# portless : 7 mins for each epoch\n",
    "model.fit(\n",
    "    x=tf_train_set, \n",
    "    validation_data=tf_validation_set, \n",
    "    epochs=3,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a9df8d48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T16:51:38.339621Z",
     "start_time": "2022-03-28T16:51:35.641099Z"
    }
   },
   "outputs": [],
   "source": [
    "# set up the git lfs thing in local machine\n",
    "answer_size = 2\n",
    "# initialize callback for best model save\n",
    "if not os.path.exists('../models'):\n",
    "    os.mkdir('../models')\n",
    "model_savepath = f'../models/{model_checkpoint}-qa-{answer_size}-ft-ar'\n",
    "\n",
    "model.save_pretrained(model_savepath)\n",
    "\n",
    "# tf.keras.models.save_model(\n",
    "#     model,\n",
    "#     filepath=model_savepath,\n",
    "#     overwrite=True,\n",
    "#     save_format=\"tf\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14ece7c",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "03fa35e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T17:11:45.826653Z",
     "start_time": "2022-03-28T17:11:44.438536Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['start_logits', 'end_logits'])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(tf_validation_set))\n",
    "output = model.predict_on_batch(batch)\n",
    "output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "53cf37cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T17:12:15.959803Z",
     "start_time": "2022-03-28T17:12:15.950821Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([138, 118, 104, 148,  97, 117, 147, 191,  97, 123, 118,  86, 162,\n",
       "         86, 179, 146]),\n",
       " array([181, 143, 159, 196, 137, 160, 189, 233, 137, 139, 143, 162, 196,\n",
       "        162, 211, 205]))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.argmax(output.start_logits, -1), np.argmax(output.end_logits, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "08ce1aff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T17:15:46.288527Z",
     "start_time": "2022-03-28T17:15:46.283641Z"
    }
   },
   "outputs": [],
   "source": [
    "n_best_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3d1c7cb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T17:15:54.047653Z",
     "start_time": "2022-03-28T17:15:54.037017Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "start_logits = output.start_logits[0]\n",
    "end_logits = output.end_logits[0]\n",
    "# Gather the indices the best start/end logits:\n",
    "start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "valid_answers = []\n",
    "for start_index in start_indexes:\n",
    "    for end_index in end_indexes:\n",
    "        if (\n",
    "            start_index <= end_index\n",
    "        ):  # We need to refine that test to check the answer is inside the context\n",
    "            valid_answers.append(\n",
    "                {\n",
    "                    \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                    \"text\": \"\",  # We need to find a way to get back the original substring corresponding to the answer in the context\n",
    "                }\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7fc4619a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T17:16:02.319305Z",
     "start_time": "2022-03-28T17:16:02.308965Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_validation_features(examples):\n",
    "    # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n",
    "    # in one example possible giving several features when a context is long, each of those features having a\n",
    "    # context that overlaps a bit the context of the previous feature.\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "    # its corresponding example. This key gives us just that.\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "\n",
    "    # We keep the example_id that gave us this feature and we will store the offset mappings.\n",
    "    tokenized_examples[\"example_id\"] = []\n",
    "\n",
    "    for i in range(len(tokenized_examples[\"input_ids\"])):\n",
    "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "        context_index = 1 if pad_on_right else 0\n",
    "\n",
    "        # One example can give several spans, this is the index of the example containing this span of text.\n",
    "        sample_index = sample_mapping[i]\n",
    "        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
    "\n",
    "        # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\n",
    "        # position is part of the context or not.\n",
    "        tokenized_examples[\"offset_mapping\"][i] = [\n",
    "            (o if sequence_ids[k] == context_index else None)\n",
    "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
    "        ]\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "47bccfad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T17:18:14.062771Z",
     "start_time": "2022-03-28T17:18:13.100598Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7a7429254a847f5b028155384029396",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'pad_on_right' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_582214/2057407415.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m validation_features = ar[\"test\"].map(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprepare_validation_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mbatched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mremove_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   1951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1952\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnum_proc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mnum_proc\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1953\u001b[0;31m             return self._map_single(\n\u001b[0m\u001b[1;32m   1954\u001b[0m                 \u001b[0mfunction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1955\u001b[0m                 \u001b[0mwith_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwith_indices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Dataset\"\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"self\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;31m# apply actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DatasetDict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    484\u001b[0m         }\n\u001b[1;32m    485\u001b[0m         \u001b[0;31m# apply actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DatasetDict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;31m# re-apply format to the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/datasets/fingerprint.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0;31m# Call actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m             \u001b[0;31m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m_map_single\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, disable_tqdm, desc, cache_only)\u001b[0m\n\u001b[1;32m   2335\u001b[0m                         )  # Something simpler?\n\u001b[1;32m   2336\u001b[0m                         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2337\u001b[0;31m                             batch = apply_function_on_filtered_inputs(\n\u001b[0m\u001b[1;32m   2338\u001b[0m                                 \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2339\u001b[0m                                 \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mapply_function_on_filtered_inputs\u001b[0;34m(inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   2216\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwith_rank\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2217\u001b[0m                 \u001b[0madditional_args\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2218\u001b[0;31m             \u001b[0mprocessed_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0madditional_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2219\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mupdate_data\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2220\u001b[0m                 \u001b[0;31m# Check if the function returns updated examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mdecorated\u001b[0;34m(item, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1911\u001b[0m                 )\n\u001b[1;32m   1912\u001b[0m                 \u001b[0;31m# Use the LazyDict internally, while mapping the function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1913\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecorated_item\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1914\u001b[0m                 \u001b[0;31m# Return a standard dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1915\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLazyDict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_582214/132532116.py\u001b[0m in \u001b[0;36mprepare_validation_features\u001b[0;34m(examples)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# context that overlaps a bit the context of the previous feature.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     tokenized_examples = tokenizer(\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mexamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"question\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpad_on_right\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"context\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mexamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"context\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpad_on_right\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"question\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"only_second\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpad_on_right\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"only_first\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pad_on_right' is not defined"
     ]
    }
   ],
   "source": [
    "validation_features = ar[\"test\"].map(\n",
    "    prepare_validation_features,\n",
    "    batched=True,\n",
    "    remove_columns=ar[\"test\"].column_names,\n",
    ")\n",
    "\n",
    "validation_dataset = validation_features.to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\"],\n",
    "    shuffle=False,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1b6f61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81701d4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d041e0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d6149ae",
   "metadata": {},
   "source": [
    "## Code for Run Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "692de144",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T17:02:34.057185Z",
     "start_time": "2022-03-28T17:02:34.052467Z"
    }
   },
   "outputs": [],
   "source": [
    "question = \"How do I stop smoking?\"\n",
    "\n",
    "text = \"Laboratory studies have also shown that emphysema and chronic bronchitis result from cigarette smoking. Even high school students who smoke have poorer lung function than nonsmokers; autopsies have shown lung damage in adults without breathing problems who stopped smoking a decade before death. Smoke causes malfunction of special white blood cells in the windpipe that guard against invading germs; smoke also slows down the microscopic hairs along the windpipe that move trapped particles up away from the lungs. Thus, it is not surprising that smokers take longer to recover from mild respiratory virus infections than nonsmokers.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0d929dba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T17:08:09.848584Z",
     "start_time": "2022-03-28T17:08:06.367825Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForQuestionAnswering: ['vocab_layer_norm', 'vocab_transform', 'vocab_projector', 'activation_13']\n",
      "- This IS expected if you are initializing TFDistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs', 'dropout_79']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_base = TFAutoModelForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")\n",
    "input_dict = tokenizer(question, text, return_tensors=\"tf\")\n",
    "outputs = model_base(input_dict)\n",
    "\n",
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits\n",
    "\n",
    "all_tokens = tokenizer.convert_ids_to_tokens(input_dict[\"input_ids\"].numpy()[0])\n",
    "answer = \" \".join(all_tokens[tf.math.argmax(start_logits, 1)[0] : tf.math.argmax(end_logits, 1)[0] + 1])\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fe7f4e9f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T17:10:08.397304Z",
     "start_time": "2022-03-28T17:10:08.390185Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['start_logits', 'end_logits'])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "65e607e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T17:04:10.052842Z",
     "start_time": "2022-03-28T17:04:09.934561Z"
    }
   },
   "outputs": [],
   "source": [
    "input_dict = tokenizer(question, text, return_tensors=\"tf\")\n",
    "outputs = model(input_dict)\n",
    "\n",
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits\n",
    "\n",
    "all_tokens = tokenizer.convert_ids_to_tokens(input_dict[\"input_ids\"].numpy()[0])\n",
    "answer = \" \".join(all_tokens[tf.math.argmax(start_logits, 1)[0] : tf.math.argmax(end_logits, 1)[0] + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f2ef087f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T17:04:12.312282Z",
     "start_time": "2022-03-28T17:04:12.305670Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3a90eadc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T17:05:28.517077Z",
     "start_time": "2022-03-28T17:05:28.506735Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'how',\n",
       " 'do',\n",
       " 'i',\n",
       " 'stop',\n",
       " 'smoking',\n",
       " '?',\n",
       " '[SEP]',\n",
       " 'laboratory',\n",
       " 'studies',\n",
       " 'have',\n",
       " 'also',\n",
       " 'shown',\n",
       " 'that',\n",
       " 'em',\n",
       " '##phy',\n",
       " '##se',\n",
       " '##ma',\n",
       " 'and',\n",
       " 'chronic',\n",
       " 'bro',\n",
       " '##nch',\n",
       " '##itis',\n",
       " 'result',\n",
       " 'from',\n",
       " 'cigarette',\n",
       " 'smoking',\n",
       " '.',\n",
       " 'even',\n",
       " 'high',\n",
       " 'school',\n",
       " 'students',\n",
       " 'who',\n",
       " 'smoke',\n",
       " 'have',\n",
       " 'poorer',\n",
       " 'lung',\n",
       " 'function',\n",
       " 'than',\n",
       " 'non',\n",
       " '##smo',\n",
       " '##kers',\n",
       " ';',\n",
       " 'auto',\n",
       " '##ps',\n",
       " '##ies',\n",
       " 'have',\n",
       " 'shown',\n",
       " 'lung',\n",
       " 'damage',\n",
       " 'in',\n",
       " 'adults',\n",
       " 'without',\n",
       " 'breathing',\n",
       " 'problems',\n",
       " 'who',\n",
       " 'stopped',\n",
       " 'smoking',\n",
       " 'a',\n",
       " 'decade',\n",
       " 'before',\n",
       " 'death',\n",
       " '.',\n",
       " 'smoke',\n",
       " 'causes',\n",
       " 'mal',\n",
       " '##fu',\n",
       " '##nction',\n",
       " 'of',\n",
       " 'special',\n",
       " 'white',\n",
       " 'blood',\n",
       " 'cells',\n",
       " 'in',\n",
       " 'the',\n",
       " 'wind',\n",
       " '##pipe',\n",
       " 'that',\n",
       " 'guard',\n",
       " 'against',\n",
       " 'invading',\n",
       " 'ge',\n",
       " '##rm',\n",
       " '##s',\n",
       " ';',\n",
       " 'smoke',\n",
       " 'also',\n",
       " 'slow',\n",
       " '##s',\n",
       " 'down',\n",
       " 'the',\n",
       " 'microscopic',\n",
       " 'hairs',\n",
       " 'along',\n",
       " 'the',\n",
       " 'wind',\n",
       " '##pipe',\n",
       " 'that',\n",
       " 'move',\n",
       " 'trapped',\n",
       " 'particles',\n",
       " 'up',\n",
       " 'away',\n",
       " 'from',\n",
       " 'the',\n",
       " 'lungs',\n",
       " '.',\n",
       " 'thus',\n",
       " ',',\n",
       " 'it',\n",
       " 'is',\n",
       " 'not',\n",
       " 'surprising',\n",
       " 'that',\n",
       " 'smoke',\n",
       " '##rs',\n",
       " 'take',\n",
       " 'longer',\n",
       " 'to',\n",
       " 'recover',\n",
       " 'from',\n",
       " 'mild',\n",
       " 'respiratory',\n",
       " 'virus',\n",
       " 'infections',\n",
       " 'than',\n",
       " 'non',\n",
       " '##smo',\n",
       " '##kers',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "63f0c60b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T17:07:24.069886Z",
     "start_time": "2022-03-28T17:07:24.063778Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 131])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.start_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cde524ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T17:07:52.771981Z",
     "start_time": "2022-03-28T17:07:52.702554Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int64, numpy=0>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.math.argmax(start_logits, 0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d5d826b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T17:07:55.509745Z",
     "start_time": "2022-03-28T17:07:55.501364Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int64, numpy=0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " tf.math.argmax(end_logits, 0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccf6286",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:msds2022-ml3]",
   "language": "python",
   "name": "conda-env-msds2022-ml3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
